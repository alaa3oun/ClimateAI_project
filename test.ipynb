{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf683d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/4000, Train Loss: 0.0135, Validation Loss: 0.0166\n",
      "Epoch 200/4000, Train Loss: 0.0085, Validation Loss: 0.0106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 104\u001b[0m\n\u001b[0;32m    100\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Train and validate the model\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m \u001b[43mtrain_validate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 74\u001b[0m, in \u001b[0;36mtrain_validate_model\u001b[1;34m(model, x_train, y_train, x_val, y_val, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     72\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x_train)\n\u001b[0;32m     73\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(output, y_train)\n\u001b[1;32m---> 74\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\USAID\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USAID\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USAID\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = \"cleaned_data.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "selected_features = [\"aet\", \"def\", \"pdsi\", \"pet\", \"soil\", \"srad\", \"tmmn\", \"tmmx\", \"vap\", \"vpd\", \"vs\"]\n",
    "df_selected = df[[\"date\"] + selected_features].copy()\n",
    "\n",
    "# Train (1975-1999) and Validation (2000-2024) Split\n",
    "train_df = df_selected[(df_selected['date'] >= '1975-01-01') & (df_selected['date'] <= '2024-12-01')]\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[selected_features])\n",
    "\n",
    "def create_sequences(data, seq_length=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 60  # 5 years\n",
    "X_train, y_train = create_sequences(train_scaled, seq_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Define LSTM Model\n",
    "class MultiOutputLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(MultiOutputLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(selected_features)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = len(selected_features)\n",
    "\n",
    "model = MultiOutputLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training and Validation Loop\n",
    "def train_model(model, x_train, y_train, criterion, optimizer, epochs=2000):\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        train_loss = criterion(output, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1)%100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    # Plot validation predictions vs. actual\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(val_years, val_actual[:, i], label=\"Actual\")\n",
    "        plt.plot(val_years, val_predictions[:, i], label=\"Predicted\", linestyle='dashed')\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(feature)\n",
    "        plt.title(f\"Validation: {feature} (2000-2024)\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "# Train and validate the model\n",
    "train_validate_model(model, X_train, y_train, X_val, y_val, criterion, optimizer, epochs=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc61b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate future dates\n",
    "def generate_future_dates(start_date, months):\n",
    "    dates = []\n",
    "    current_date = start_date\n",
    "    for _ in range(months):\n",
    "        # Handle month increment\n",
    "        year = current_date.year + (current_date.month // 12)\n",
    "        month = current_date.month % 12 + 1\n",
    "        current_date = current_date.replace(year=year, month=month)\n",
    "        dates.append(current_date)\n",
    "    return dates\n",
    "\n",
    "# Predict future time steps given a user-defined end date\n",
    "def predict_future(model, train_df, scaler, selected_features, seq_length, target_end_date_str):\n",
    "    model.eval()\n",
    "\n",
    "    # Get the last sequence from the training data\n",
    "    recent_data = train_df[selected_features].values[-seq_length:]\n",
    "    input_seq = torch.tensor(recent_data, dtype=torch.float32).unsqueeze(0)  # Shape: (1, seq_len, features)\n",
    "\n",
    "    # Parse the target end date\n",
    "    target_end_date = pd.to_datetime(target_end_date_str)\n",
    "    last_known_date = train_df[\"date\"].iloc[-1]\n",
    "    \n",
    "    # Calculate number of months to predict\n",
    "    months_to_predict = (target_end_date.year - last_known_date.year) * 12 + (target_end_date.month - last_known_date.month)\n",
    "    if months_to_predict <= 0:\n",
    "        raise ValueError(\"Target date must be after the last date in the training set.\")\n",
    "\n",
    "    predictions = []\n",
    "    future_dates = []\n",
    "\n",
    "    for _ in range(months_to_predict):\n",
    "        with torch.no_grad():\n",
    "            next_pred = model(input_seq).squeeze(0).numpy()  # Predict next step\n",
    "        predictions.append(next_pred)\n",
    "        future_dates.append(last_known_date + pd.DateOffset(months=len(future_dates)+1))\n",
    "\n",
    "        # Update input sequence\n",
    "        next_input = np.vstack([input_seq.squeeze(0).numpy()[1:], next_pred])\n",
    "        input_seq = torch.tensor(next_input, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    prediction_df = pd.DataFrame(predictions, columns=selected_features)\n",
    "    prediction_df[\"date\"] = future_dates\n",
    "\n",
    "    return prediction_df\n",
    "\n",
    "# After training:\n",
    "# Example usage\n",
    "user_input_date = \"2029-12-01\"  # or input(\"Enter prediction end date (YYYY-MM-DD): \")\n",
    "predicted_df = predict_future(model, train_df, scaler, selected_features, seq_length, user_input_date)\n",
    "\n",
    "# Save or plot\n",
    "predicted_df.to_csv(\"predicted_future.csv\", index=False)\n",
    "print(predicted_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
